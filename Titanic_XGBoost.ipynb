{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Titanic Kaggle: XGBoost Model\n",
        "This notebook demonstrates a commonly used approach for tackling the Titanic classification problem on Kaggle.\n",
        "\n",
        "Steps:\n",
        "1. Load libraries\n",
        "2. Load data\n",
        "3. Exploratory Data Analysis & Feature Engineering\n",
        "4. Model building with XGBoost\n",
        "5. Evaluation (local CV) and Prediction\n",
        "6. Generate submission file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.max_rows', 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data\n",
        "Make sure you have `train.csv` and `test.csv` from the Titanic Kaggle competition in the same folder as this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load train and test data\n",
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')\n",
        "\n",
        "print(\"Train shape:\", train_data.shape)\n",
        "print(\"Test shape:\\t\", test_data.shape)\n",
        "\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Exploratory Data Analysis & Basic Cleaning\n",
        "- Check missing values\n",
        "- Fill or drop as needed\n",
        "- Convert categorical variables\n",
        "- Feature engineering (Title, FamilySize, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check missing values in train\n",
        "train_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see Age and Cabin are quite missing, and Embarked might have some missing. The Cabin column often gets dropped or encoded in various ways. Let’s do a minimal approach here.\n",
        "\n",
        "**Key feature engineering steps** often used:\n",
        "1. **Title extraction**: from Name (e.g., Mr, Mrs, Miss, Master)\n",
        "2. **Fill missing Age** with median (optionally by Title)\n",
        "3. **Family size**: SibSp + Parch + 1 (the passenger)\n",
        "4. **Embarked**: fill with mode (most common)\n",
        "5. **Fare**: fill missing (in test) with median, or do binning\n",
        "6. **Cabin**: either drop or turn into an indicator for having a cabin vs. not\n",
        "7. **Drop unnecessary columns** like Ticket, or partial: Name, Cabin.\n",
        "\n",
        "We’ll demonstrate a typical pipeline below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine train and test for consistent feature engineering\n",
        "full_data = pd.concat([train_data, test_data], sort=False).reset_index(drop=True)\n",
        "full_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Extract Title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_title(name):\n",
        "    # Extract title from the name\n",
        "    # e.g., 'Braund, Mr. Owen Harris' -> 'Mr'\n",
        "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
        "    if title_search:\n",
        "        return title_search.group(1)\n",
        "    return \"\"  # in case there's no title\n",
        "\n",
        "full_data['Title'] = full_data['Name'].apply(get_title)\n",
        "\n",
        "# Some titles are very rare and can be grouped\n",
        "full_data['Title'] = full_data['Title'].replace(['Lady','Countess','Capt','Col',\\\n",
        "                                                'Don','Dr','Major','Rev','Sir',\\\n",
        "                                                'Jonkheer','Dona'],'Rare')\n",
        "full_data['Title'] = full_data['Title'].replace('Mlle','Miss')\n",
        "full_data['Title'] = full_data['Title'].replace('Ms','Miss')\n",
        "full_data['Title'] = full_data['Title'].replace('Mme','Mrs')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Fill Missing Data\n",
        "- **Age**: fill with median, optionally by Title.\n",
        "- **Embarked**: fill missing with mode (S)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fill Embarked\n",
        "full_data['Embarked'].fillna('S', inplace=True)\n",
        "\n",
        "# Fill Fare (if missing in test)\n",
        "full_data['Fare'].fillna(full_data['Fare'].median(), inplace=True)\n",
        "\n",
        "# Let's fill Age by median of [Title]\n",
        "age_medians = full_data.groupby('Title')['Age'].median()\n",
        "\n",
        "def fill_age(row):\n",
        "    if pd.isnull(row['Age']):\n",
        "        return age_medians[row['Title']]\n",
        "    else:\n",
        "        return row['Age']\n",
        "\n",
        "full_data['Age'] = full_data.apply(fill_age, axis=1)\n",
        "\n",
        "# We'll drop Cabin but create a feature \"HasCabin\" (0 or 1)\n",
        "full_data['HasCabin'] = full_data['Cabin'].apply(lambda x: 0 if pd.isnull(x) else 1)\n",
        "\n",
        "# Now drop columns we won't use\n",
        "full_data.drop(['PassengerId','Name','Cabin','Ticket'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Create Family Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "full_data['FamilySize'] = full_data['SibSp'] + full_data['Parch'] + 1\n",
        "full_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Convert Categorical to Numeric\n",
        "We'll encode **Sex** (already M/F) and **Embarked**, **Title** if not numeric.\n",
        "\n",
        "Actually, **Sex** is male/female, so we can map that to 0/1. Or we can treat it as a string and let XGBoost handle it, but typically we do numeric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode Sex as 0/1\n",
        "full_data['Sex'] = full_data['Sex'].map({'male':1, 'female':0}).astype(int)\n",
        "\n",
        "# Embarked can also be encoded\n",
        "le_embarked = LabelEncoder()\n",
        "full_data['Embarked'] = le_embarked.fit_transform(full_data['Embarked'])\n",
        "\n",
        "# Title\n",
        "le_title = LabelEncoder()\n",
        "full_data['Title'] = le_title.fit_transform(full_data['Title'])\n",
        "\n",
        "full_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 Split back into train/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = full_data[:len(train_data)]\n",
        "test_df = full_data[len(train_data):]\n",
        "\n",
        "# Our target is Survived in train_data\n",
        "y = train_data['Survived'].values\n",
        "X = train_df.values\n",
        "\n",
        "X_test = test_df.values\n",
        "\n",
        "print(\"Train shape:\", X.shape)\n",
        "print(\"Test shape:\\t\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Building (XGBoost)\n",
        "We’ll do a quick local validation with `StratifiedKFold` or `train_test_split`.\n",
        "\n",
        "We can do some light hyperparameter tuning. Often, people use Bayesian optimization or larger grid searches for the best parameters. Here we demonstrate a smaller approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# We'll do a simple train/val split for local check\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, \\ \n",
        "                                                  random_state=42, stratify=y)\n",
        "\n",
        "# Build the model\n",
        "model = xgb.XGBClassifier(\n",
        "    n_estimators=1000,\n",
        "    max_depth=4,\n",
        "    learning_rate=0.01,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss'  # for binary classification\n",
        ")\n",
        "\n",
        "# Fit with early stopping\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    early_stopping_rounds=50,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "val_preds = model.predict(X_val)\n",
        "val_acc = accuracy_score(y_val, val_preds)\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can try adjusting parameters like `max_depth`, `learning_rate`, etc., to see if you get better local validation accuracy. Typical local validation accuracies with a well-tuned XGBoost for Titanic might be **0.82–0.86**.\n",
        "\n",
        "If you want a more robust measure, you can do **k-fold cross validation**. But for demonstration, a single split is fine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train on Full Training Set & Generate Submission\n",
        "Now that we’ve done a local check, we can train on the **entire** dataset and produce predictions on the test set for Kaggle submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Retrain on the entire training data\n",
        "model_full = xgb.XGBClassifier(\n",
        "    n_estimators=1000,\n",
        "    max_depth=4,\n",
        "    learning_rate=0.01,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "model_full.fit(X, y)\n",
        "\n",
        "# Predict on test data\n",
        "test_preds = model_full.predict(X_test)\n",
        "test_preds[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Create Submission File\n",
        "Kaggle expects a CSV with format:\n",
        "\n",
        "```\n",
        "PassengerId,Survived\n",
        "892,0\n",
        "893,1\n",
        "...\n",
        "```\n",
        "\n",
        "But we previously dropped PassengerId. We’ll fetch it from the original test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "submission = pd.read_csv('test.csv', usecols=['PassengerId'])\n",
        "submission['Survived'] = test_preds.astype(int)\n",
        "submission.to_csv('submission_xgb.csv', index=False)\n",
        "submission.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can now **upload** this `submission_xgb.csv` to the Titanic Kaggle competition. Typical XGBoost solutions with decent feature engineering often score in the **0.78–0.82** range on the public leaderboard (sometimes higher if well-tuned)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Possible Improvements\n",
        "1. **More Feature Engineering**: Create advanced features from `Cabin` or `Name` (Title grouping is just a start). Compute family survival rates if using the complementary test set, etc.\n",
        "2. **Hyperparameter Tuning**: Use cross-validation or a tuning library (Optuna, GridSearchCV, etc.) to find the best parameters (max_depth, learning_rate, n_estimators, etc.).\n",
        "3. **Ensembling**: Combine multiple models (e.g., random forest, LightGBM, logistic regression) to get a better final prediction.\n",
        "4. **Handling Outliers**: If any outlier is present in `Fare` or `Age`, apply transformations or binning.\n",
        "5. **Stacking**: Build multiple levels of models (meta-model) for improved performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Conclusion\n",
        "In this notebook, we:\n",
        "1. Loaded and preprocessed the Titanic dataset\n",
        "2. Engineered features (Title, HasCabin, FamilySize)\n",
        "3. Trained a strong baseline model using XGBoost\n",
        "4. Produced a CSV submission file for Kaggle\n",
        "\n",
        "This approach typically scores well on the leaderboard. You can further refine and tune hyperparameters for even better performance."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9+",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
